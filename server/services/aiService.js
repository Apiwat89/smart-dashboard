// server/services/aiService.js
const OpenAI = require("openai");
require('dotenv').config();

// ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ô‡πÇ‡∏´‡∏°‡∏î DEV ‡∏´‡∏£‡∏∑‡∏≠ PROD
const isDev = process.env.NODE_ENV !== 'production';

// ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Client
// ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dev -> ‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ localhost:11434 (Ollama)
// ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Prod -> ‡∏¢‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ api.openai.com (‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á)
const aiClient = new OpenAI({
    baseURL: isDev ? 'http://localhost:11434/v1' : 'https://api.openai.com/v1',
    apiKey: isDev ? 'ollama' : process.env.OPENAI_API_KEY, 
});

async function generateAIResponse(userMessage, systemRole = "You are a helpful assistant.") {
    try {
        console.log(`ü§ñ AI Processing... (Mode: ${isDev ? 'Ollama/Local' : 'OpenAI/Cloud'})`);
        
        const response = await aiClient.chat.completions.create({
            // ‡∏ä‡∏∑‡πà‡∏≠ Model: ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ Ollama ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô llama3.2, llama3, qwen)
            model: isDev ? 'llama3.2' : 'gpt-4o-mini', 
            messages: [
                { role: 'system', content: systemRole },
                { role: 'user', content: userMessage }
            ],
            temperature: 0.7, // ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (0-1)
            max_tokens: 200,  // ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        });

        return response.choices[0].message.content;

    } catch (error) {
        console.error("AI Error Connection:", error.message);
        
        if (error.code === 'ECONNREFUSED') {
            return "Error: ‡∏•‡∏∑‡∏°‡πÄ‡∏õ‡∏¥‡∏î Ollama ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏Ñ‡∏£‡∏±‡∏ö? (Connection Refused)";
        }
        return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏°‡∏≠‡∏á‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß";
    }
}

module.exports = { generateAIResponse };